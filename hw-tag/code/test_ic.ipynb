{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3"]},{"cell_type":"markdown","metadata":{},"source":["This file illustrates how you might experiment with the HMM interface.\n","You can paste these commands in at the Python prompt, or execute `test_ic.py` directly.\n","A notebook interface is nicer than the plain Python prompt, so we provide\n","a notebook version of this file as `test_ic.ipynb`, which you can open with\n","`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import logging, math, os\n","from pathlib import Path"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from corpus import TaggedCorpus\n","from eval import model_cross_entropy, write_tagging\n","from hmm import HiddenMarkovModel\n","from crf import ConditionalRandomField"]},{"cell_type":"markdown","metadata":{},"source":["Set up logging."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["log = logging.getLogger(\"test_ic\")       # For usage, see findsim.py in earlier assignment.\n","logging.root.setLevel(level=logging.INFO)\n","logging.basicConfig(level=logging.INFO)  # could change INFO to DEBUG\n","# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"]},{"cell_type":"markdown","metadata":{},"source":["Switch working directory to the directory where the data live.  You may want to edit this line."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["os.chdir(\"../data\")"]},{"cell_type":"markdown","metadata":{},"source":["Get vocabulary and tagset from a supervised corpus."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:corpus:Read 40 tokens from icsup\n","INFO:corpus:Created 4 tag types\n","INFO:corpus:Created 5 word types\n","INFO:test_ic:Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n","INFO:test_ic:Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"]}],"source":["icsup = TaggedCorpus(Path(\"icsup\"), add_oov=False)\n","log.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n","log.info(f\"Ice cream tagset: {list(icsup.tagset)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Two ways to look at the corpus ..."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n","1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n","1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n","1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n"]},{"data":{"text/plain":["0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["os.system(\"cat icsup\")   # call the shell to look at the file directly"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n","1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n","1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n","1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n"]}],"source":["log.info(icsup)          # print the TaggedCorpus python object we constructed from it"]},{"cell_type":"markdown","metadata":{},"source":["Make an HMM."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Hidden Markov Model (HMM) test\n","\n","INFO:test_ic:*** Current A, B matrices (using initalizations from the ice cream spreadsheet)\n"]},{"name":"stdout","output_type":"stream","text":["Transition matrix A:\n","\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n","C\t0.800\t0.100\t0.100\t0.000\n","H\t0.100\t0.800\t0.100\t0.000\n","_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n","_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n","\n","Emission matrix B:\n","\t1\t2\t3\n","C\t0.700\t0.200\t0.100\n","H\t0.100\t0.200\t0.700\n","_EOS_TAG_\t0.000\t0.000\t0.000\n","_BOS_TAG_\t0.000\t0.000\t0.000\n","\n","\n"]}],"source":["log.info(\"*** Hidden Markov Model (HMM) test\\n\")\n","hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab)\n","# Change the transition/emission initial probabilities to match the ice cream spreadsheet,\n","# and test your implementation of the Viterbi algorithm.  Note that the spreadsheet \n","# uses transposed versions of these matrices.\n","hmm.B = tensor([[0.7000, 0.2000, 0.1000],    # emission probabilities\n","                [0.1000, 0.2000, 0.7000],\n","                [0.0000, 0.0000, 0.0000],\n","                [0.0000, 0.0000, 0.0000]])\n","hmm.A = tensor([[0.8000, 0.1000, 0.1000, 0.0000],   # transition probabilities\n","                [0.1000, 0.8000, 0.1000, 0.0000],\n","                [0.0000, 0.0000, 0.0000, 0.0000],\n","                [0.5000, 0.5000, 0.0000, 0.0000]])\n","log.info(\"*** Current A, B matrices (using initalizations from the ice cream spreadsheet)\")\n","hmm.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Try it out on the raw data from the spreadsheet, available in `icraw``."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Viterbi results on icraw with hard coded parameters\n","100%|██████████| 1/1 [00:00<00:00, 122.00it/s]"]},{"name":"stdout","output_type":"stream","text":["2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["0"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["log.info(\"*** Viterbi results on icraw with hard coded parameters\")\n","icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n","write_tagging(hmm, icraw, Path(\"icraw_hmm.output\"))  # calls hmm.viterbi_tagging on each sentence\n","os.system(\"cat icraw_hmm.output\")   # print the file we just created, and remove it\n"]},{"cell_type":"markdown","metadata":{},"source":["Did the parameters that we guessed above get the \"correct\" answer, \n","as revealed in `icdev`?"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Compare to icdev corpus:\n","2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/C 3/C 3/C 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n","100%|██████████| 1/1 [00:00<00:00, 117.49it/s]\n","INFO:eval:Tagging accuracy: all: 90.909%, seen: 90.909%, novel: nan%\n"]},{"data":{"text/plain":["0.09090909090909094"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["icdev = TaggedCorpus(Path(\"icdev\"), tagset=icsup.tagset, vocab=icsup.vocab)\n","log.info(f\"*** Compare to icdev corpus:\\n{icdev}\")\n","from eval import viterbi_error_rate\n","viterbi_error_rate(hmm, icdev, show_cross_entropy=False)"]},{"cell_type":"markdown","metadata":{},"source":["Now let's try your training code, running it on supervised data.\n","To test this, we'll restart from a random initialization.\n","(You could also try creating this new model with `unigram=true`, \n","which will affect the rest of the notebook.)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** A, B matrices as randomly initialized close to uniform\n"]},{"name":"stdout","output_type":"stream","text":["Transition matrix A:\n","\tC\tH\t_EOS_TAG_\t_BOS_TAG_\n","C\t0.334\t0.334\t0.332\t0.000\n","H\t0.334\t0.332\t0.334\t0.000\n","_EOS_TAG_\t0.334\t0.333\t0.333\t0.000\n","_BOS_TAG_\t0.333\t0.334\t0.334\t0.000\n","\n","Emission matrix B:\n","\t1\t2\t3\n","C\t0.333\t0.335\t0.332\n","H\t0.333\t0.333\t0.334\n","_EOS_TAG_\t0.000\t0.000\t0.000\n","_BOS_TAG_\t0.000\t0.000\t0.000\n","\n","\n"]}],"source":["hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab)\n","log.info(\"*** A, B matrices as randomly initialized close to uniform\")\n","hmm.printAB()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:test_ic:*** Supervised training on icsup\n","100%|██████████| 4/4 [00:00<00:00, 469.20it/s]\n","INFO:eval:Cross-entropy: inf nats (= perplexity inf)\n","  0%|          | 0/4 [00:00<?, ?it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing sentence: [(4, 3), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (2, 1), (3, 2)]\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=3, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=0, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=1, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=0, word=1, count=1\n","Added transition count: prev_tag=0, tag=0, count=1\n","Added emission count: tag=1, word=2, count=1\n","Added transition count: prev_tag=0, tag=1, count=1\n"]},{"ename":"IndexError","evalue":"index 3 is out of bounds for dimension 1 with size 3","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Supervised training on icsup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m cross_entropy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m model: model_cross_entropy(model, icsup)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mhmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43micsup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** A, B matrices after training on icsup (should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch initial params on spreadsheet [transposed])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m hmm\u001b[38;5;241m.\u001b[39mprintAB()\n","File \u001b[0;32m~/jhu/senior_year/nlp/nlp-hw-6/NLP_HW6/hw-tag/code/hmm.py:267\u001b[0m, in \u001b[0;36mHiddenMarkovModel.train\u001b[0;34m(self, corpus, loss, λ, tolerance, max_steps, save_path)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(corpus, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(corpus), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    266\u001b[0m     isent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_integerize_sentence(sentence, corpus)\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mE_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43misent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# M step: Update the parameters based on the accumulated counts.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM_step(λ)\n","File \u001b[0;32m~/jhu/senior_year/nlp/nlp-hw-6/NLP_HW6/hw-tag/code/hmm.py:328\u001b[0m, in \u001b[0;36mHiddenMarkovModel.E_step\u001b[0;34m(self, isent, mult)\u001b[0m\n\u001b[1;32m    324\u001b[0m prev_word_id, prev_tag_id \u001b[38;5;241m=\u001b[39m isent[j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# we have a tag\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# Accumulate emission count\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB_counts[tag_id, word_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mult\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded emission count: tag=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, word=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, count=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# Accumulate transition count if we have previous tag\u001b[39;00m\n","\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 1 with size 3"]}],"source":["log.info(\"*** Supervised training on icsup\")\n","cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n","hmm.train(corpus=icsup, loss=cross_entropy_loss, tolerance=0.0001)\n","log.info(\"*** A, B matrices after training on icsup (should \"\n","         \"match initial params on spreadsheet [transposed])\")\n","hmm.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Now that we've reached the spreadsheet's starting guess, let's again tag\n","the spreadsheet \"sentence\" (that is, the sequence of ice creams) using the\n","Viterbi algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Viterbi results on icraw\")\n","icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n","write_tagging(hmm, icraw, Path(\"icraw_hmm.output\"))  # calls hmm.viterbi_tagging on each sentence\n","os.system(\"cat icraw_hmm.output\")   # print the file we just created, and remove it"]},{"cell_type":"markdown","metadata":{},"source":["Next let's use the forward algorithm to see what the model thinks about \n","the probability of the spreadsheet \"sentence.\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n","             \"on spreadsheet)\")\n","for sentence in icraw:\n","    prob = math.exp(hmm.logprob(sentence, icraw))\n","    log.info(f\"{prob} = p({sentence})\")"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let's reestimate on the icraw data, as the spreadsheet does.\n","We'll evaluate as we go along on the *training* perplexity, and stop\n","when that has more or less converged."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n","negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n","hmm.train(corpus=icraw, loss=negative_log_likelihood, tolerance=0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** A, B matrices after reestimation on icraw\"\n","         \"should match final params on spreadsheet [transposed])\")\n","hmm.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Now let's try out a randomly initialized CRF on the ice cream data. Notice how\n","the initialized A and B matrices now hold non-negative potentials,\n","rather than probabilities that sum to 1."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Conditional Random Field (CRF) test\\n\")\n","crf = ConditionalRandomField(icsup.tagset, icsup.vocab)\n","log.info(\"*** Current A, B matrices (potentials from small random parameters)\")\n","crf.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Now let's try your training code, running it on supervised data. To test this,\n","we'll restart from a random initialization. \n","\n","Note that the logger reports the CRF's *conditional* cross-entropy, \n","log p(tags | words) / n.  This is much lower than the HMM's *joint* \n","cross-entropy log p(tags, words) / n, but that doesn't mean the CRF\n","is worse at tagging.  The CRF is just predicting less information."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Supervised training on icsup\")\n","cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n","crf.train(corpus=icsup, loss=cross_entropy_loss, lr=0.1, tolerance=0.0001)\n","log.info(\"*** A, B matrices after training on icsup\")\n","crf.printAB()"]},{"cell_type":"markdown","metadata":{},"source":["Let's again tag the spreadsheet \"sentence\" (that is, the sequence of ice creams) \n","using the Viterbi algorithm (this may not match the HMM)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log.info(\"*** Viterbi results on icraw with trained parameters\")\n","icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n","write_tagging(hmm, icraw, Path(\"icraw_crf.output\"))  # calls hmm.viterbi_tagging on each sentence\n","os.system(\"cat icraw_crf.output\")   # print the file we just created, and remove it"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":2}
